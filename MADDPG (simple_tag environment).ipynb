{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Requirements\n",
    "\n",
    "`>> pip install git+https://github.com/openai/multiagent-particle-envs.git`\n",
    "\n",
    "`>> pip install gym==0.10.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment description\n",
    "\n",
    "There are 3 adversary agents (red), 1 nonadversary agent (green) and 2 landmarks (black). Nonadversary agent is penalized based on how far the agent is from adversary agents. Nonadversary agent get penalty if the agent collide with adversary agents. Adversary agents get reward if they collide with nonadversary agent. So, nonadversary agent have to learn to run away, and adversary agent have to learn to catch.\n",
    "\n",
    "## 1.1 State\n",
    "Each agent's state is n-dimensional state, with following components:\n",
    "\n",
    "### 1.1.1 adversary\n",
    "<center>[agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + other_vel</center>\n",
    "    \n",
    "- `agent.state.p_vel` : x, y speed (2-dim)\n",
    "- `agent.state.p_pos` : x, y position (2-dim)\n",
    "- `entity_pos` : x, y position of landmarks (2 * 2 dim)\n",
    "- `other_pos`: x, y poisition of other agents (2 * 3 dim)\n",
    "- `other_vel` : x, y speed of nonadversary agent\n",
    "- total 16 dimensional state\n",
    "\n",
    "### 1.1.2 nonadversary\n",
    "<center>[agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos</center>\n",
    "    \n",
    "- `agent.state.p_vel` : x, y speed (2-dim)\n",
    "- `agent.state.p_pos` : x, y position (2-dim)\n",
    "- `entity_pos` : x, y position of landmarks (2 * 2 dim)\n",
    "- `other_pos`: x, y poisition of other agents (2 * 3 dim)\n",
    "- total 14 dimensional state\n",
    "\n",
    "## 1.2 Action\n",
    "\n",
    "Agent's action is 5-dimensional, where each element implies the velocity of the direction:\n",
    "\n",
    "<center>[hold, right, left, up, down]</center>\n",
    "\n",
    "For example, if action is [0, 1, 0, 0, 1], the agent is moving to the north-east direction with speed $\\sqrt{2}/\\text{timestep}$ .\n",
    "\n",
    "Nonadversary agent's maximum speed is twice of adversary agents'.\n",
    "\n",
    "## 1.3 Reward\n",
    "\n",
    "Modified version of the original simple-spread environment (see `simple_spread.py` file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_tag import Scenario\n",
    "\n",
    "def make_env():\n",
    "\n",
    "    from multiagent.environment import MultiAgentEnv\n",
    "\n",
    "    # load scenario from script\n",
    "    scenario = Scenario()\n",
    "    \n",
    "    # create world\n",
    "    world = scenario.make_world()\n",
    "    \n",
    "    # create multiagent environment\n",
    "    env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)\n",
    "        \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- environment check ----------\n",
      "agents' action space      :  [5, 5, 5, 5]\n",
      "agents' observation space :  [16, 16, 16, 14]\n",
      "Is agent an adversary?    :  [True, True, True, False]\n"
     ]
    }
   ],
   "source": [
    "env = make_env()\n",
    "\n",
    "print(\"---------- environment check ----------\")\n",
    "\n",
    "print(\"agents' action space      : \", [a_s.n for a_s in env.action_space])\n",
    "\n",
    "print(\"agents' observation space : \", [o_s.shape[0] for o_s in env.observation_space])\n",
    "\n",
    "print(\"Is agent an adversary?    : \", [env.agents[i].adversary for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Networks\n",
    "## 2.1 Multi-Layered-Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, hidden_dims=[64, 64], hidden_act=nn.ReLU(), out_act=nn.Identity()):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "\n",
    "        in_dims = [in_dim] + hidden_dims\n",
    "        out_dims = hidden_dims + [out_dim]\n",
    "\n",
    "        for _in, _out in zip(in_dims, out_dims):\n",
    "            self.layers.append(nn.Linear(_in, _out))\n",
    "\n",
    "        for i in range(len(hidden_dims)):\n",
    "            self.activations.append(hidden_act)\n",
    "        self.activations.append(out_act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l, a in zip(self.layers, self.activations):\n",
    "            x = l(x)\n",
    "            x = a(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Agent\n",
    "\n",
    "## 3.1 Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, length):\n",
    "        self.memory = deque(maxlen=length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, size):\n",
    "        sample = random.sample(self.memory, size)\n",
    "\n",
    "        state = [i[0] for i in sample]\n",
    "        action = [i[1] for i in sample]\n",
    "        reward = [i[2] for i in sample]\n",
    "        next_state = [i[3] for i in sample]\n",
    "        terminal = [i[4][0] for i in sample]\n",
    "\n",
    "        return state, action, reward, next_state, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Update model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(source, target, tau):\n",
    "    \n",
    "    for src_param, target_param in zip(source.parameters(), target.parameters()):\n",
    "        target_param.data.copy_(tau * src_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.policy = MLP(state_dim, action_dim, out_act=nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.policy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=32):\n",
    "        super(Critic, self).__init__()\n",
    "        self.bef_embed_action_net = MLP(state_dim, hidden_dim, out_act=nn.ReLU())\n",
    "        self.aft_embed_action_net = MLP(hidden_dim + action_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = self.bef_embed_action_net(state)\n",
    "        embed_action = torch.cat([x, action], dim=-1)\n",
    "        x = self.aft_embed_action_net(embed_action)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, action_min, action_max, state_dim_all, action_dim_all, gamma=0.99):\n",
    "        super(DDPG_Agent, self).__init__()\n",
    "        self.action_min = np.array(action_min)\n",
    "        self.action_max = np.array(action_max)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim_all, action_dim_all)\n",
    "\n",
    "        self.actor_target = Actor(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim_all, action_dim_all)\n",
    "\n",
    "        update_model(self.actor, self.actor_target, tau=1.0)\n",
    "        update_model(self.critic, self.critic_target, tau=1.0)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=0.0001)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=0.001)\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        action = self.actor(state).detach()\n",
    "\n",
    "        action = torch.clamp(action + epsilon * (torch.rand(5) * 2.0 - 1.0), min=-1.0, max=1.0)\n",
    "        action_norm = action * self.action_max\n",
    "\n",
    "        return action, action_norm\n",
    "\n",
    "    def update_target(self, tau=0.05):\n",
    "\n",
    "        update_model(self.actor, self.actor_target, tau=tau)\n",
    "        update_model(self.critic, self.critic_target, tau=tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 MADDPG Agent\n",
    "\n",
    "Pseudocode of MADDPG is as follow.\n",
    "\n",
    "![pseudocode.png](images/pseudocode.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG_Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_agents, state_dim, action_dim, action_min, action_max, state_dim_all, action_dim_all, gamma=0.99):\n",
    "        super(MADDPG_Agent, self).__init__()\n",
    "        \n",
    "        self.number_of_agents = num_agents\n",
    "        self.replay_memory = ReplayMemory(50000)\n",
    "        self.batch_size = 50\n",
    "        \n",
    "        self.agents = []\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.00005\n",
    "        self.epsilon_min = 0.1\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            temp_agent = DDPG_Agent(state_dim[i], action_dim[i], action_min[i], action_max[i], state_dim_all, action_dim_all, gamma)\n",
    "            self.agents.append(temp_agent)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \n",
    "        actions = []\n",
    "        actions_norm = []\n",
    "        \n",
    "        for i in range(self.number_of_agents):\n",
    "            action, action_norm = self.agents[i].get_action(state[i], self.epsilon)\n",
    "            actions.append(action.detach())\n",
    "            actions_norm.append(action_norm.tolist())\n",
    "        \n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        \n",
    "        return actions, actions_norm\n",
    "    \n",
    "    def update_model(self):\n",
    "        \n",
    "        for agent_idx in range(self.number_of_agents):\n",
    "            self.agents[agent_idx].update_target()\n",
    "            \n",
    "    def push(self, transition):\n",
    "        self.replay_memory.push(transition)\n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        a_loss_sum = 0.0\n",
    "        c_loss_sum = 0.0\n",
    "        \n",
    "        if len(self.replay_memory) > self.batch_size:\n",
    "            \n",
    "            for agent_idx in range(self.number_of_agents):\n",
    "                \n",
    "                s, a, r, s_, t = self.replay_memory.sample(self.batch_size)\n",
    "                \n",
    "                c_target_x = []\n",
    "                c_target_a = []\n",
    "                \n",
    "                c_x = torch.stack([torch.cat(s_elem) for s_elem in s]).type(torch.float32)\n",
    "                c_a = torch.tensor(np.stack([np.concatenate(a_elem) for a_elem in a]), dtype=torch.float32)                \n",
    "\n",
    "                c_orig_x = c_x.clone().detach()\n",
    "                c_orig_a = []\n",
    "                \n",
    "                for batch_idx in range(batch_size):\n",
    "\n",
    "                    x_ = s_[batch_idx]\n",
    "                    x_ = torch.cat(x_)\n",
    "                    c_target_x.append(x_)\n",
    "                    \n",
    "                    a_next_s = [self.agents[idx].actor_target(s_[batch_idx][idx]) for idx in range(4)]\n",
    "                    a_next_s = torch.cat(a_next_s)\n",
    "                    c_target_a.append(a_next_s)\n",
    "                    \n",
    "                    a_cur_s = [self.agents[idx].actor(s[batch_idx][idx]) for idx in range(4)]\n",
    "                    a_cur_s = torch.cat(a_cur_s)\n",
    "                    c_orig_a.append(a_cur_s)\n",
    "                    \n",
    "                c_target_x = torch.stack(c_target_x)\n",
    "                c_target_a = torch.stack(c_target_a)\n",
    "                c_orig_a = torch.stack(c_orig_a)\n",
    "                \n",
    "                r = torch.tensor([r_elem[agent_idx] for r_elem in r]).reshape(-1, 1)\n",
    "                t = torch.tensor(t, dtype=torch.float32)\n",
    "                t = (1 - t).reshape(-1, 1)\n",
    "                \n",
    "                # target & predicted value\n",
    "                y = r + t * gamma * self.agents[agent_idx].critic_target(c_target_x, c_target_a)\n",
    "                y = y.detach()\n",
    "                q_value = self.agents[agent_idx].critic(c_x, c_a)\n",
    "                \n",
    "                # update critic network\n",
    "                c_loss = self.agents[agent_idx].mse_loss(y, q_value)\n",
    "                self.agents[agent_idx].critic_optimizer.zero_grad()\n",
    "                c_loss.backward()\n",
    "                self.agents[agent_idx].critic_optimizer.step()\n",
    "\n",
    "                # update actor network\n",
    "                a_loss = - self.agents[agent_idx].critic(c_orig_x, c_orig_a)\n",
    "                a_loss = a_loss.mean()\n",
    "                self.agents[agent_idx].actor_optimizer.zero_grad()\n",
    "                a_loss.backward()\n",
    "                self.agents[agent_idx].actor_optimizer.step()\n",
    "                \n",
    "                c_loss_sum += c_loss.item()\n",
    "                a_loss_sum += a_loss.item()\n",
    "        \n",
    "        return a_loss_sum, c_loss_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(data, title, file_name):\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.savefig(file_name)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Define agent and envirionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- agents information ----------\n",
      "agent 0 : state dim : 16, action dim :5\n",
      "agent 1 : state dim : 16, action dim :5\n",
      "agent 2 : state dim : 16, action dim :5\n",
      "agent 3 : state dim : 14, action dim :5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = make_env()\n",
    "\n",
    "episodes = 5\n",
    "episode_max_len = 60\n",
    "gamma = 0.99\n",
    "\n",
    "number_of_agents = env.n\n",
    "agents = []\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "state_dim = []\n",
    "action_dim = []\n",
    "\n",
    "state_dim_all = 16 * 3 + 14\n",
    "action_dim_all = 5 * 4\n",
    "\n",
    "action_min = []\n",
    "action_max = []\n",
    "\n",
    "print(\"---------- agents information ----------\")\n",
    "for i in range(number_of_agents):\n",
    "\n",
    "    state_dim_i = env.observation_space[i].shape[0]\n",
    "    action_dim_i = env.action_space[i].n\n",
    "\n",
    "    if env.agents[i].adversary:\n",
    "        action_low, action_high = [-1.0] * action_dim_i, [1.0] * action_dim_i\n",
    "    else:\n",
    "        action_low, action_high = [-2.0] * action_dim_i, [2.0] * action_dim_i\n",
    "    \n",
    "    state_dim.append(state_dim_i)\n",
    "    action_dim.append(action_dim_i)\n",
    "    \n",
    "    action_min.append(action_low)\n",
    "    action_max.append(action_high)\n",
    "\n",
    "    print(\"agent %d : state dim : %d, action dim :%d\" % (i, state_dim_i, action_dim_i))\n",
    "print(\"\")\n",
    "\n",
    "Agents = MADDPG_Agent(num_agents = number_of_agents, state_dim = state_dim, action_dim = action_dim, \n",
    "                action_min = action_min, action_max = action_max, state_dim_all = state_dim_all, action_dim_all = action_dim_all, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Train MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  0\n",
      "-0.3181, -0.3181, -0.3181, 0.2002, epsilon : 1.0000\n",
      "-0.3264, -0.3264, -0.3264, 0.3264, epsilon : 0.9999\n",
      "-0.3440, -0.3440, -0.3440, 0.3440, epsilon : 0.9999\n",
      "-0.3335, -0.3335, -0.3335, 0.3335, epsilon : 0.9998\n",
      "-0.3025, -0.3025, -0.3025, 0.3025, epsilon : 0.9998\n",
      "-0.3101, -0.3101, -0.3101, 0.3101, epsilon : 0.9997\n",
      "-0.3156, -0.3156, -0.3156, 0.3156, epsilon : 0.9997\n",
      "-0.3100, -0.3100, -0.3100, 0.3100, epsilon : 0.9996\n",
      "-0.2735, -0.2735, -0.2735, 0.2735, epsilon : 0.9996\n",
      "-0.2713, -0.2713, -0.2713, 0.2713, epsilon : 0.9995\n",
      "-0.2632, -0.2632, -0.2632, 0.2632, epsilon : 0.9995\n",
      "-0.2680, -0.2680, -0.2680, 0.2680, epsilon : 0.9994\n",
      "-0.3059, -0.3059, -0.3059, 0.3059, epsilon : 0.9994\n",
      "-0.3207, -0.3207, -0.3207, 0.3207, epsilon : 0.9993\n",
      "-0.3315, -0.3315, -0.3315, 0.3315, epsilon : 0.9993\n",
      "-0.3349, -0.3349, -0.3349, 0.3349, epsilon : 0.9992\n",
      "-0.3368, -0.3368, -0.3368, 0.3368, epsilon : 0.9992\n",
      "-0.3578, -0.3578, -0.3578, 0.3578, epsilon : 0.9991\n",
      "-0.3756, -0.3756, -0.3756, 0.3756, epsilon : 0.9991\n",
      "-0.3969, -0.3969, -0.3969, 0.3969, epsilon : 0.9990\n",
      "-0.4199, -0.4199, -0.4199, 0.4199, epsilon : 0.9990\n",
      "-0.4490, -0.4490, -0.4490, 0.4490, epsilon : 0.9989\n",
      "-0.4667, -0.4667, -0.4667, 0.4667, epsilon : 0.9989\n",
      "-0.4668, -0.4668, -0.4668, 0.4668, epsilon : 0.9988\n",
      "-0.4808, -0.4808, -0.4808, 0.4706, epsilon : 0.9988\n",
      "-0.5108, -0.5108, -0.5108, -0.2532, epsilon : 0.9987\n",
      "-0.5389, -0.5389, -0.5389, -0.5715, epsilon : 0.9987\n",
      "-0.5747, -0.5747, -0.5747, -0.7802, epsilon : 0.9986\n",
      "-0.6033, -0.6033, -0.6033, -0.9324, epsilon : 0.9986\n",
      "-0.6055, -0.6055, -0.6055, -0.8110, epsilon : 0.9985\n",
      "-0.6161, -0.6161, -0.6161, -0.7358, epsilon : 0.9985\n",
      "-0.6331, -0.6331, -0.6331, -0.6040, epsilon : 0.9984\n",
      "-0.6620, -0.6620, -0.6620, -0.5935, epsilon : 0.9984\n",
      "-0.6834, -0.6834, -0.6834, -0.6603, epsilon : 0.9983\n",
      "-0.7260, -0.7260, -0.7260, -1.0152, epsilon : 0.9983\n",
      "-0.7614, -0.7614, -0.7614, -1.4042, epsilon : 0.9982\n",
      "-0.7851, -0.7851, -0.7851, -1.6967, epsilon : 0.9982\n",
      "-0.7936, -0.7936, -0.7936, -1.6331, epsilon : 0.9981\n",
      "-0.8146, -0.8146, -0.8146, -1.5855, epsilon : 0.9981\n",
      "-0.8387, -0.8387, -0.8387, -1.4942, epsilon : 0.9980\n",
      "-0.8353, -0.8353, -0.8353, -1.7262, epsilon : 0.9980\n",
      "-0.8293, -0.8293, -0.8293, -1.9631, epsilon : 0.9979\n",
      "-0.8212, -0.8212, -0.8212, -1.8487, epsilon : 0.9979\n",
      "-0.8202, -0.8202, -0.8202, -1.8228, epsilon : 0.9978\n",
      "-0.8423, -0.8423, -0.8423, -2.0501, epsilon : 0.9978\n",
      "-0.8196, -0.8196, -0.8196, -1.7880, epsilon : 0.9977\n",
      "-0.8074, -0.8074, -0.8074, -1.5661, epsilon : 0.9977\n",
      "-0.8057, -0.8057, -0.8057, -1.6154, epsilon : 0.9976\n",
      "-0.8130, -0.8130, -0.8130, -1.8236, epsilon : 0.9976\n",
      "-0.8131, -0.8131, -0.8131, -1.3546, epsilon : 0.9975\n",
      "-0.8148, -0.8148, -0.8148, -1.1365, epsilon : 0.9975\n",
      "-0.8143, -0.8143, -0.8143, -1.1805, epsilon : 0.9974\n",
      "-0.8203, -0.8203, -0.8203, -1.5240, epsilon : 0.9974\n",
      "-0.8326, -0.8326, -0.8326, -1.6951, epsilon : 0.9973\n",
      "-0.8744, -0.8744, -0.8744, -2.2924, epsilon : 0.9973\n",
      "-0.9060, -0.9060, -0.9060, -3.1722, epsilon : 0.9972\n",
      "-0.9200, -0.9200, -0.9200, -4.1008, epsilon : 0.9972\n",
      "-0.9278, -0.9278, -0.9278, -5.0205, epsilon : 0.9971\n",
      "-0.9177, -0.9177, -0.9177, -4.4874, epsilon : 0.9971\n",
      "-0.9363, -0.9363, -0.9363, -5.5842, epsilon : 0.9970\n",
      "episode :  1\n",
      "-0.2495, -0.2495, -0.2495, 0.2495, epsilon : 0.9970\n",
      "-0.2557, -0.2557, -0.2557, 0.2557, epsilon : 0.9969\n",
      "-0.2635, -0.2635, -0.2635, 0.2635, epsilon : 0.9969\n",
      "-0.2754, -0.2754, -0.2754, 0.2754, epsilon : 0.9968\n",
      "-0.2726, -0.2726, -0.2726, 0.2726, epsilon : 0.9968\n",
      "-0.2757, -0.2757, -0.2757, 0.2757, epsilon : 0.9967\n",
      "-0.2741, -0.2741, -0.2741, 0.2741, epsilon : 0.9967\n",
      "-0.2791, -0.2791, -0.2791, 0.2791, epsilon : 0.9966\n",
      "-0.2912, -0.2912, -0.2912, 0.2912, epsilon : 0.9966\n",
      "-0.3092, -0.3092, -0.3092, 0.3092, epsilon : 0.9965\n",
      "-0.3148, -0.3148, -0.3148, 0.3148, epsilon : 0.9965\n",
      "-0.3067, -0.3067, -0.3067, 0.3067, epsilon : 0.9964\n",
      "-0.3017, -0.3017, -0.3017, 0.3017, epsilon : 0.9964\n",
      "-0.3071, -0.3071, -0.3071, 0.3071, epsilon : 0.9963\n",
      "-0.3136, -0.3136, -0.3136, 0.3136, epsilon : 0.9963\n",
      "-0.3303, -0.3303, -0.3303, 0.3303, epsilon : 0.9962\n",
      "-0.3483, -0.3483, -0.3483, 0.3483, epsilon : 0.9962\n",
      "-0.3734, -0.3734, -0.3734, -0.4567, epsilon : 0.9961\n",
      "-0.3964, -0.3964, -0.3964, -0.7713, epsilon : 0.9961\n",
      "-0.4237, -0.4237, -0.4237, -0.9694, epsilon : 0.9960\n",
      "-0.4556, -0.4556, -0.4556, -1.3096, epsilon : 0.9960\n",
      "-0.4622, -0.4622, -0.4622, -1.2165, epsilon : 0.9959\n",
      "-0.4864, -0.4864, -0.4864, -1.3423, epsilon : 0.9959\n",
      "-0.4921, -0.4921, -0.4921, -1.2461, epsilon : 0.9958\n",
      "-0.4826, -0.4826, -0.4826, -0.9857, epsilon : 0.9958\n",
      "-0.4622, -0.4622, -0.4622, -0.6731, epsilon : 0.9957\n",
      "-0.4621, -0.4621, -0.4621, -0.5758, epsilon : 0.9957\n",
      "-0.4618, -0.4618, -0.4618, -0.5414, epsilon : 0.9956\n",
      "-0.4620, -0.4620, -0.4620, -0.3118, epsilon : 0.9956\n",
      "-0.4666, -0.4666, -0.4666, -0.0505, epsilon : 0.9955\n",
      "-0.4560, -0.4560, -0.4560, 0.2017, epsilon : 0.9955\n",
      "-0.4479, -0.4479, -0.4479, -0.2750, epsilon : 0.9954\n",
      "-0.4467, -0.4467, -0.4467, -0.6578, epsilon : 0.9954\n",
      "-0.4452, -0.4452, -0.4452, -0.8215, epsilon : 0.9953\n",
      "-0.4402, -0.4402, -0.4402, -0.9324, epsilon : 0.9953\n",
      "-0.4361, -0.4361, -0.4361, -1.0259, epsilon : 0.9952\n",
      "-0.4303, -0.4303, -0.4303, -1.0066, epsilon : 0.9952\n",
      "9.5660, 9.5660, 9.5660, -11.1752, epsilon : 0.9951\n",
      "9.5535, 9.5535, 9.5535, -11.9045, epsilon : 0.9951\n",
      "-0.4770, -0.4770, -0.4770, -2.4464, epsilon : 0.9950\n",
      "-0.5116, -0.5116, -0.5116, -2.6935, epsilon : 0.9950\n",
      "-0.5271, -0.5271, -0.5271, -2.6143, epsilon : 0.9949\n",
      "-0.5643, -0.5643, -0.5643, -3.1175, epsilon : 0.9949\n",
      "-0.5786, -0.5786, -0.5786, -3.4450, epsilon : 0.9948\n",
      "-0.5976, -0.5976, -0.5976, -3.8154, epsilon : 0.9948\n",
      "-0.6197, -0.6197, -0.6197, -4.2170, epsilon : 0.9947\n",
      "-0.6542, -0.6542, -0.6542, -4.8677, epsilon : 0.9947\n",
      "-0.6794, -0.6794, -0.6794, -5.4522, epsilon : 0.9946\n",
      "-0.7015, -0.7015, -0.7015, -6.2296, epsilon : 0.9946\n",
      "-0.7080, -0.7080, -0.7080, -6.7711, epsilon : 0.9945\n",
      "-0.6928, -0.6928, -0.6928, -5.9233, epsilon : 0.9945\n",
      "-0.6741, -0.6741, -0.6741, -4.9952, epsilon : 0.9944\n",
      "-0.6547, -0.6547, -0.6547, -4.3296, epsilon : 0.9944\n",
      "-0.6442, -0.6442, -0.6442, -4.2012, epsilon : 0.9943\n",
      "-0.6192, -0.6192, -0.6192, -3.2245, epsilon : 0.9943\n",
      "-0.6010, -0.6010, -0.6010, -2.4365, epsilon : 0.9942\n",
      "-0.5886, -0.5886, -0.5886, -1.9096, epsilon : 0.9942\n",
      "-0.5770, -0.5770, -0.5770, -1.5917, epsilon : 0.9941\n",
      "-0.5718, -0.5718, -0.5718, -1.4217, epsilon : 0.9941\n",
      "-0.5768, -0.5768, -0.5768, -1.1018, epsilon : 0.9940\n",
      "episode :  2\n",
      "-0.2851, -0.2851, -0.2851, 0.2851, epsilon : 0.9940\n",
      "-0.2609, -0.2609, -0.2609, 0.2609, epsilon : 0.9939\n",
      "-0.2496, -0.2496, -0.2496, 0.2496, epsilon : 0.9939\n",
      "-0.2464, -0.2464, -0.2464, 0.2464, epsilon : 0.9938\n",
      "-0.2145, -0.2145, -0.2145, 0.2145, epsilon : 0.9938\n",
      "9.8081, 9.8081, 9.8081, -9.8081, epsilon : 0.9937\n",
      "9.8182, 9.8182, 9.8182, -9.8182, epsilon : 0.9937\n",
      "-0.1926, -0.1926, -0.1926, 0.1926, epsilon : 0.9936\n",
      "-0.2084, -0.2084, -0.2084, -0.2284, epsilon : 0.9936\n",
      "-0.1982, -0.1982, -0.1982, -0.0646, epsilon : 0.9935\n",
      "-0.2212, -0.2212, -0.2212, -0.8387, epsilon : 0.9935\n",
      "-0.2289, -0.2289, -0.2289, -0.9111, epsilon : 0.9934\n",
      "-0.2017, -0.2017, -0.2017, -0.8164, epsilon : 0.9934\n",
      "-0.2024, -0.2024, -0.2024, -0.9348, epsilon : 0.9933\n",
      "-0.1881, -0.1881, -0.1881, -0.9100, epsilon : 0.9933\n",
      "-0.1734, -0.1734, -0.1734, -0.8004, epsilon : 0.9932\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-adcbc066b143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mAgents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-8e661cc569ad>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mc_target_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_next_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                     \u001b[0ma_cur_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                     \u001b[0ma_cur_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_cur_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                     \u001b[0mc_orig_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_cur_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-8e661cc569ad>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mc_target_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_next_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                     \u001b[0ma_cur_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                     \u001b[0ma_cur_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_cur_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                     \u001b[0mc_orig_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_cur_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-47807866427c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-9094a29ee22c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "actor_loss_all = []\n",
    "critic_loss_all = []\n",
    "\n",
    "rewards = [[], [], [], []]\n",
    "\n",
    "for epi in range(episodes):\n",
    "    \n",
    "    print(\"episode : \", epi)\n",
    "\n",
    "    state = env.reset()\n",
    "    state = [torch.tensor(s, dtype=torch.float32) for s in state]\n",
    "\n",
    "    r = [0, 0, 0, 0]\n",
    "\n",
    "    for time in range(episode_max_len):\n",
    "        \n",
    "        action, action_norm = Agents.get_action(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action_norm)\n",
    "        next_state = [torch.tensor(s, dtype=torch.float32) for s in next_state]\n",
    "        \n",
    "        print('{:4.4f}, {:4.4f}, {:4.4f}, {:4.4f}, epsilon : {:.4f}'.format(reward[0], reward[1], reward[2], reward[3], Agents.epsilon))\n",
    "        \n",
    "        for i in range(number_of_agents):\n",
    "            r[i] += reward[i]\n",
    "\n",
    "        if time + 1 == episode_max_len:\n",
    "            done = [True for _ in range(Agents.number_of_agents)]\n",
    "\n",
    "        transition = [state, action, reward, next_state, done]\n",
    "        Agents.push(transition)\n",
    "        state = next_state\n",
    "        \n",
    "        actor_loss, critic_loss = Agents.train_model()\n",
    "        Agents.update_model()\n",
    "        \n",
    "        actor_loss_all.append(actor_loss)\n",
    "        critic_loss_all.append(critic_loss)\n",
    "\n",
    "        if all(done):\n",
    "            break\n",
    "    \n",
    "    for i in range(number_of_agents):\n",
    "        rewards[i].append(r[i])\n",
    "        \n",
    "    plot_graph(actor_loss_all, 'actor loss', 'images/a_loss.png')\n",
    "    plot_graph(critic_loss_all, 'critic loss', 'images/c_loss.png')\n",
    "    plot_graph(rewards[0], 'reward 1', 'images/reward1.png')\n",
    "    plot_graph(rewards[1], 'reward 2', 'images/reward2.png')\n",
    "    plot_graph(rewards[2], 'reward 3', 'images/reward3.png')\n",
    "    plot_graph(rewards[3], 'reward 4', 'images/reward4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualize\n",
    "\n",
    "## 4.1 Visualizing state function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def vis_state(env):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # msking rectangle\n",
    "    ax.set_xlim([-1.5, 1.5])\n",
    "    ax.set_ylim([-1.5, 1.5])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # plotting agent and landmarks\n",
    "    for entity in env.world.entities:\n",
    "        color = entity.color\n",
    "        size = entity.size\n",
    "        center = entity.state.p_pos\n",
    "        draw_circle = plt.Circle(center, size, color=color)\n",
    "        ax.add_artist(draw_circle)\n",
    "    plt.show()\n",
    "    clear_output(wait=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Checking the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16a86906438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_episodes = 1\n",
    "max_timestep = 40\n",
    "n_agents = env.n\n",
    "action_dim = env.action_space[0].n\n",
    "\n",
    "\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    \n",
    "    t = 0\n",
    "    terminated = False\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = [torch.tensor(s, dtype=torch.float32) for s in state]\n",
    "\n",
    "    while True:             \n",
    "        vis_state(env)\n",
    "        t += 1\n",
    "        action, action_norm = Agents.get_action(state)\n",
    "        \n",
    "        next_state, reward, terminated, _ = env.step(action_norm)\n",
    "        \n",
    "        state = next_state\n",
    "        state = [torch.tensor(s, dtype=torch.float32) for s in state]\n",
    "        \n",
    "        if all(terminated) or t >= max_timestep:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
